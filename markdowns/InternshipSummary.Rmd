---
title: "Modelling the Stock-Recruitment relationship"
author: "Todor Dimitrov"
date: "8/28/2019"
output: html_document
bibliography: references.bib
csl: fisheries-research.csl
params:
  simYrs: 30
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('rjags')
options(knitr.table.format = 'markdown')
```

## Introduction
 <br />
Estimating and modeling the stock-recruitment relationship has been a central issue in fishery dynamics. Over the years, a wide variety of suggestions and solutions have appeared to fit and estimate critical parameters. These span from drawing recruitment curves by eye to a strictly statistical procedure of curve fitting [@walters1981effects]. While each approach has its own validity and use, Bayesian methodology and state-space modelling have become the prominent focus of more recent literature [@clark2004population]. Through this paper, we aim to highlight the benefits of combining both methodologies with regards to fishery population dynamics.
 <br />
 
### State-space Framework
 <br />
As models have developed over time, focus has shifted from simply fitting a curve to assuring that key sources of variability have been considered and included [@walters1981effects]. In particular, the importance of both observation error and process variability has become increasingly recognized within the fishery literature [@ahrestani2013importance]. Observation error, as one would expect, results from variability introduced by the methods used to obtain population data. Source of observation error include sampling error as well as human error and harsh environmental conditions. On the other hand, process variability models the random variability in the state equation being used [@meyer1999bayesian].A state-space model aims to separate the two sources of variability in order to better estimate population parameters such as productivity, capacity and process variability variance. 
 <br />
  
#### Basic State-space model
 <br />
Let $x_t$ be a population state at time $t$ and $y_t$ represent the observation made. The state-space model is as follows:
 <br /> <br />
 <center>
  $x_t = f(x_{t-1}) + \epsilon_t$
 <br />
  $y_t = g(x_t) + \omega_t$
  </center>
 <br />
Through this framework, we are able of separating the two sources of variability: $\epsilon_t$ (process variability) and $\omega_t$ (observation error). The first equation is our process equation while the second is our observation equation. With some minor alterations, this framework will be the guiding principle behind our stock-recruitment model. In particular, we will focus on the log densities of the states ($x_t = ln(n_t)$). This, with the assumptions that $\epsilon_t \sim N(0, \sigma^2)$ and $\omega_t \sim N(0, \tau^2)$ will give us a lognormal distribution for both the process and observation models. In terms of functions we will use the Beverton-Holt equation for the process model describing the state ($f$). The Berverton-Holt is parameterized in terms of the productivity (slope at origin), and capacity of the population. There are many other choices, such as the Ricker model, but for our study we decided to focus on the Beverton-Holt function. For the observation model, $g$, we used the identity function.
 <br /> <br />
<center>$f(S) = \frac{S}{\frac{1}{p} + \frac{S}{c}}$</center>
 <br />
<center>$g(x) = x$</center>
 <br />
Here $p$ represents productivity, $c$ is capacity, and $S$ is the spawner count. Using this basic framework, we can model both observation error and process variability. 
 <br />
 
## Models

In this section we will:
 <br />
1. Fit a non state-space model to simulated data.
 <br />
2. Fit a state-space model to simulated data.
 <br />
3. Investigate the effects of different levels of simulate process error on the performance of the modeling fitting for 1 and 2 above.
 <br />

### Data Generation
First, to illustrate and be able to test the validity of our models, we needed a testing data set. Using the state space model outlined in the previous section, we simulated `r params$simYrs` years of population data. We used an initial state of 1000 individuals in the population with a capacity of 1200 and a productivity of 2.5. We assumed process variability standard deviation was 0.1 ($\epsilon_t = 0.1$). We added a known harvest rate ($HR$), ranging from 0 to 0.63, and a percentage of hatchery origin fish which spawned in the river ($pHOS$). This value ranged from 0 to 0.9. This allowed us to more realistically simulate salmon population data and provided a mechanism for creating contrast in the number of spawners which improved model fit. After we simulated the true state, $y_t$ above, we added observation error to generate the observed data, $x_t$, where ($\omega_t = 0.15$).  

Process Model:
```{r process model}
bevholt = function(S, prod, cap) {
  S/(1/prod+S/cap)
}

set.seed(123)
N <- params$simYrs
pHOS <- sample(rep(seq(0, .9, .1), rep(N/10,10)))
HR <- sample(rep(seq(0, .63, .07), rep(N/10,10)))
S <- numeric(N)
S[1] <- 1000
cap <- 1200
prod <- 2.5
for (i in 1:(N-1)){
  S[i+1] <- rlnorm(1, log(bevholt(S[i], prod, cap)*(1-HR[i])/(1-pHOS[i])), .1)
}
```

Observation Model:
```{r obs model}
Sobs <- numeric(N)
for (i in 1:N) {
  Sobs[i] <- rlnorm(1, log(S[i]), .15)
}
```

The following plots are of the 'actual' simulated states, $y_t$, and the observed data, $x_t$:
```{r data plot,out.width = '75%', fig.height= 9, fig.align = "center",fig.width=6, fig.height=12}
par(mfrow=c(2,1))

Spr <- S[-N]
Rec <- S[-1]*(1-pHOS[-N])/(1-HR[-N])
plot(Spr,Rec, xlim = c(0,max(Spr)*1.1), ylim = c(0,max(Rec)*1.1), xlab="Spawner", ylab="Recruits", main="Simulated spawners (true state that is not observed)", bty="l")
Spr <- Sobs[-N]
Rec <- Sobs[-1]*(1-pHOS[-N])/(1-HR[-N])
plot(Spr,Rec, xlim = c(0,max(Spr)*1.1), ylim = c(0,max(Rec)*1.1), xlab="Spawner", ylab="Recruits", main="Observed Data", bty="l")
```

### Simple Model (Non State-space)

We start by using the traditional approach to fitting the spawner-recruit function to the data where observation error and process variability are modeled separately. 

When using a Bayesian modeling framework, parameters that are estimated require prior distributions. These priors, describe the apriori assumptions of different parameter values. Because the modeling results can be sensitive to the piors, part of creating a accurate Bayesian population dynamics model is careful consideration of the priors. For some parameters, guidance can be found in the literature [@millar2002reference]. In our case, since we simulated our data, we are assume that the following are reasonable priors (notice this is comparable to using prior knowledge of population specific biology to develop informative priors):
<center>
$pro \sim lognormal(log(3), 0.01)$
<br />
$cap \sim lognormal(log(15000), 0.001))$
<br />
$\tau \sim gamma(0.001, 0.001)$
</center>
Here, $\tau = \frac{1}{\sigma^2}$
<br />
Notice that we are using the precision, $\tau = 1/\sigma^2$ because the the normal distribution is parameterized this way in the JAGS/BUGS language.
<br />
Now, using JAGS, we can create our simple model. Here, we attempt to model using only the observed data.
<br />
```{r jags code}
simple.bug <- '
model {
  for (i in 1:(Nyears-1)) {
    R[i] ~ dlnorm(log(mu[i]), tau)
    mu[i] <- S[i]/(1/prod + S[i]/cap)
  }
  
  prod ~ dlnorm(log(3), 0.01)   
  cap ~ dlnorm(log(15000), 0.001)
  tau ~ dgamma(0.001, 0.001) 
}
'
```
<br />
We are then able to run the model with initial values of prod = 1 and cap = 600. Here, we assume that the harvest rate, $HR$, and proportion of naturally spawning hatchery fish, $pHOS$ is known. We use this to transform the data into spawners, $S$, and recruits, $R$, where spawners is the total number of spawning fish that may contribute to the next generation and recruits is the total number of fish that would have returned if there was no harvest or hatchery origin fish.
<br />
```{r fit simple model, cache=TRUE}
initValFunc <- function() {
  list(prod=1, cap=600)
}
jagsDat <- list(S=Sobs[-N], R=Sobs[-1]*(1-pHOS[-N])/(1-HR[-N]), Nyears=N)
m.simple <- jags.model(textConnection(simple.bug), data=jagsDat, initValFunc, n.chains=3)
s.simple <- jags.samples(m.simple,n.iter=10000,variable.names=c("prod","cap", "tau"))
prodLi <- s.simple$prod
capLi <- s.simple$cap
sigmaLi <- sqrt(1/s.simple$tau)
U <- 1-1/sqrt(prodLi)
median(prodLi)
median(capLi)
```
<br />
This model generates a reasonable results. By sampling from the posterior distribution of the parameters, we can see the median of productivity and capacity are around the set values we used to generate the data. To get a sense of the wide range of plausible curves, we can plot multiple Beverton-Holt curves using sample capacities and productivity. The 1:1 reference line was plotted as well.
<br />
```{r plot simple model fit, fig.width=6, fig.height=6}
par(mfrow = c(1,1))
Spr <- Sobs[-N]
Rec <- Sobs[-1]*(1-pHOS[-N])/(1-HR[-N])
plot(Spr,Rec, xlim = c(0,max(Spr)*1.1), ylim = c(0,max(Rec)*1.1), xlab="Spawner", ylab="Recruits", main="Simple Model",bty="l")
SS <- 0:10000
Ndraws <- 100
col <- rgb(1,0,0,0.1)
for (i in 1:Ndraws) {
  draw <- sample(1:length(prodLi), 1)
  lines(SS, bevholt(SS, prodLi[draw], capLi[draw]), type = "l", col = col, lwd = .5)  
}
lines(SS, bevholt(SS, median(prodLi), median(capLi)), type = "l", col = 1, lwd = 4)  
lines(SS, SS,lty=3)
```
<br />
A value which fisheries managers are often interested in is the harvest rate at maximum sustainable yield ($U$), or the harvest rate at which the largest possible catch can be extracted while resulting in the same number of spawners in the next generation. From past literature we know that $U$ for the Beverton-Holt curve is calculated as $1-\frac{1}{\sqrt {prod}}$ [@hilborn1992quantitative]. Using this formula, we can find $U$ for all the samples from the posterior. Calculating the 0.025 and 0.975 quantiles of these values results in a 95% credible interval.
<br />
<center>
``` {r MSY simple}
hist(U, main = "MSY of simple model")
abline(v = quantile(U,prob=c(0.025,0.5,0.975)), col="red", lwd=3, lty=2)
quantile(prodLi,prob=c(0.025,0.5,0.975))
```
</center>
<br />
Using the same approach we can calculate credible intervals for the productivity and capacity parameters.
<br />
<center>
``` {r Prod and cap simple, echo=FALSE}
par(mfrow = c(1,2))
hist(prodLi, main = "Productivity of simple model")
abline(v = quantile(prodLi,prob=c(0.025,0.5,0.975)), col="red", lwd=3, lty=2)
hist(capLi, main = "Capacity of simple model")
abline(v = quantile(capLi,prob=c(0.025,0.5,0.975)), col="red", lwd=3, lty=2)
```
</center>
<br />
Ultimately, what we see is that the simple model does a relatively accurate job of bounding the parameters as well as estimating values such as U. However, once again, this model was under ideal conditions (large quantities of data, relatively small error) and does not incorporate or take into account the different forms of error which occur from generating these observations.
<br />

### State-based Model

<br />
Now, we will look at a state-based model under the same conditions as the simple model. We are using the same generated data and priors as above. However, now, in our model, we will explicitly model the process and the observation. Here, we will assume that observation error has been measured and is known ($\omega_t = 0.15$).
<br />

```{r jags state code}
state.bug <- '
model {
  S[1] ~ dlnorm(0,0.0001)
  for(i in 1:(Nyears-1)) {
    S[i+1] ~ dlnorm(log((S[i]/(1/prod + S[i]/cap))*(1/(1-pHOS[i]))*(1-HR[i])), tau)
  }
  
  for (i in 1:Nyears) {
    Sobs[i] ~ dlnorm(log(S[i]),1/(.15*.15))
  }
  
  prod ~ dlnorm(log(3), 0.01)   
  cap ~ dlnorm(log(15000), 0.001)
  tau ~ dgamma(0.001, 0.001)
}
'
```
<br />
Using a similar process as above we can run the model with initial values of prod = 1 and cap = 600.
<br />
```{r fit state-space model, cache=TRUE}
jagsDat <- list(Sobs=Sobs, Nyears=N, pHOS=pHOS, HR=HR)
m.state <- jags.model(textConnection(state.bug), data=jagsDat, initValFunc, n.chains=3, n.adapt=5000)
s.state <- jags.samples(m.state,n.iter=10000,variable.names=c("prod","cap", "tau", "S"))
prodLi <- s.state$prod
capLi <- s.state$cap
sigmaLi <- sqrt(1/s.state$tau)
U <- 1-1/sqrt(prodLi)
median(prodLi)
median(capLi)
```
<br />
A sample of parameters values from the posterior were used to plot Beverton-Holt curves to characterize uncertainty in the model fit.
<br />
```{r plot state-space model fit, echo=FALSE, fig.width=6, fig.height=6}
par(mfrow = c(1,1))
Spr <- Sobs[-N]
Rec <- Sobs[-1]*(1-pHOS[-N])/(1-HR[-N])
plot(Spr,Rec, xlim = c(0,max(Spr)*1.1), ylim = c(0,max(Rec)*1.1), xlab="Spawner", ylab="Recruits", main="State-space model",bty="l")

SS <- 0:10000
Ndraws <- 100
for (i in 1:Ndraws) {
  draw <- sample(1:length(prodLi), 1)
  lines(SS, bevholt(SS, prodLi[draw], capLi[draw]), type = "l", col = col, lwd = .5)  
}
lines(SS, bevholt(SS, median(prodLi), median(capLi)), type = "l", col = 1, lwd = 4)  
lines(SS, SS,lty=3)
```
<br />
A summary of the the parameter posterior distributions as well as $U$ can be quickly calculated. Once again, the 95% credible intervals as well as the medians of each distributions are plotted. First the calculated values.
<br />
<center>
``` {r MSY state, echo=FALSE}
hist(U, main = "U, harvest rate at MSY of simple model")
abline(v = quantile(U,prob=c(0.025,0.5,0.975)), col="red", lwd=3, lty=2)
quantile(prodLi,prob=c(0.025,0.5,0.975))
```
</center>
<br />
The parameters:
<br />
<center>
``` {r Prod and cap state, echo=FALSE}
par(mfrow = c(1,2))
hist(prodLi, main = "Productivity of simple model")
abline(v = quantile(prodLi,prob=c(0.025,0.5,0.975)), col="red", lwd=3, lty=2)
hist(capLi, main = "Capacity of simple model")
abline(v = quantile(capLi,prob=c(0.025,0.5,0.975)), col="red", lwd=3, lty=2)
```
</center>
<br />
The credible intervals for the parameters are a bit tighter around the true values, however, under these ideal conditions, both models do a accurate job of estimating the parameter values.

### Varying Process variability

<br />
We will now take a look at how varying the process variability effects each models' accuracy for predicting parameters. To perform this, we generate data using a similar process as seen above, simply varying the used process variability from $0.1$ to $0.7$. In fisheries modeling, it is common to have substantial variability due to natural processes that can not be explained (for example variable ocean survival). The following graphs and table summarize the results from varying the process variability.
<br />
```{r process variability variation, echo=FALSE, results='hide', cache=TRUE}
set.seed(123)
N <- params$simYrs
pHOS <- sample(rep(seq(0, .9, .1), rep(N/10,10)))
HR <- sample(rep(seq(0, .63, .07), rep(N/10,10)))
S <- numeric(N)
PErr <- seq(.1, .7, .1)
dataState <- numeric(35)
dataCompStat <- matrix(1:(30000*14),30000,14)
SobsDat <- matrix(1:(N*7), N, 7)
for (j in 1:7) {
  S[1] <- 1000
  cap <- 1200
  prod <- 2.5
  for (i in 1:(N-1)){
    S[i+1] <- rlnorm(1, log(bevholt(S[i], prod, cap)*(1-HR[i])/(1-pHOS[i])), PErr[j])
  }
  
  #observation model
  Sobs <- numeric(N)
  for (i in 1:N) {
    Sobs[i] <- rlnorm(1, log(S[i]), .15)
  }
  SobsDat[1:N,j] <- Sobs
}

for (j in 1:7) {
  jagsDat <- list(Sobs=SobsDat[1:N,j], Nyears=N, pHOS=pHOS, HR=HR)
  m1 <- jags.model(textConnection(state.bug), data=jagsDat, initValFunc, n.chains=3, n.adapt=5000)
  s1 <- jags.samples(m1,n.iter=10000,variable.names=c("prod","cap", "tau", "S"))
  prodLi <- s1$prod
  capLi <- s1$cap
  sigmaLi <- sqrt(1/s1$tau)
  dataCompStat[1:30000,2*j] <- capLi
  dataCompStat[1:30000,2*j-1] <- prodLi
  dataState[5*j] <- median(sigmaLi)
  dataState[5*j-1] <- sd(capLi)
  dataState[5*j-2] <- sd(prodLi)
  dataState[5*j-3] <- median(capLi)
  dataState[5*j-4] <- median(prodLi) 
}
tState <- matrix(dataState,ncol=7,byrow=FALSE)
colnames(tState) <- c("1","2","3","4","5","6","7")
rownames(tState) <- c("Prod Median", "Cap Median", "Prod sd", "Cap sd", "Estimated process variability")
tState <- as.table(tState)

dataSimple <- numeric(28)
dataCompSimp <- matrix(1:(30000*14),30000,14)
for (j in 1:7) {
  Sobs <- SobsDat[1:N,j]
  jagsDat <- list(S=Sobs[-N], R=Sobs[-1]*(1-pHOS[-N])/(1-HR[-N]), Nyears=N)
  m2 <- jags.model(textConnection(simple.bug), data=jagsDat, initValFunc, n.chains=3, n.adapt=5000)
  s2 <- jags.samples(m2,n.iter=10000,variable.names=c("prod","cap"))
  prodLi <- s2$prod
  capLi <- s2$cap
  dataCompSimp[1:30000,2*j] <- capLi
  dataCompSimp[1:30000,2*j-1] <- prodLi
  dataSimple[4*j] <- sd(capLi)
  dataSimple[4*j-1] <- sd(prodLi)
  dataSimple[4*j-2] <- median(capLi)
  dataSimple[4*j-3] <- median(prodLi) 
}
tSimple <- matrix(dataSimple,ncol=7,byrow=FALSE)
colnames(tSimple) <- c("1","2","3","4","5","6","7")
rownames(tSimple) <- c("Prod Median", "Cap Median", "Prod sd", "Cap sd")
tSimple <- as.table(tSimple)

```

<center>
```{r process variability variation plots, echo=FALSE, fig.width=8, fig.height=4}
par(mfrow = c(1,2))
for (j in 1:7) {
  prodLi <- dataCompStat[1:30000,2*j-1]
  capLi <- dataCompStat[1:30000,2*j]
  Sobs <- SobsDat[1:N,j]
  Spr <- Sobs[-N]
  Rec <- Sobs[-1]*(1-pHOS[-N])/(1-HR[-N])
  plot(Spr,Rec, xlim = c(0,max(Spr)*1.1), ylim = c(0,max(Rec)*1.1), xlab="Spawner", ylab="Recruits", main=paste("State-space: procSD=",PErr[j],sep=""),bty="l")
  SS <- 0:20000
  Ndraws <- 100
  for (i in 1:Ndraws) {
    draw <- sample(1:length(prodLi), 1)
    lines(SS, bevholt(SS, prodLi[draw], capLi[draw]), type = "l", col = col, lwd = .5)  
  }
  lines(SS, bevholt(SS, median(prodLi), median(capLi)), type = "l", col = 1, lwd = 4)  
  lines(SS, SS,lty=3)
  
  prodLi <- dataCompSimp[1:30000,2*j-1]
  capLi <- dataCompSimp[1:30000,2*j]
  plot(Spr,Rec, xlim = c(0,max(Spr)*1.1), ylim = c(0,max(Rec)*1.1), xlab="Spawner", ylab="Recruits", main=paste("Simple: procSD=",PErr[j],sep=""),bty="l")
  
  SS <- 0:20000
  Ndraws <- 100
  for (i in 1:Ndraws) {
    draw <- sample(1:length(prodLi), 1)
    lines(SS, bevholt(SS, prodLi[draw], capLi[draw]), type = "l", col = col, lwd = .5)  
  }
  lines(SS, bevholt(SS, median(prodLi), median(capLi)), type = "l", col = 1, lwd = 4)  
  lines(SS, SS,lty=3)
}
```
<br />
State Model
<br />
```{r state table, echo=FALSE}
knitr::kable(
  tState, caption = 'State-space model Results. Includes the median and standard deviation for the different parameters.',digits=2
)
```
<br />
Simple Model
<br />
```{r simple table, echo=FALSE}
knitr::kable(
  tSimple, caption = 'Simple Model Results. Includes the median and standard deviation for the different parameters.',digits=2
)
```
</center>

<br />
From the tables above we can see that both models struggle to fit the curves when process variability gets large. This can also be seen in the graphs by the larger spread of the drawn curves. However, the clear advantage of the state-based approach is that it does an accurate job of modeling the process variability. In the simple model, if we were to estimate process error, our best estimate would likely be total error since we are unable to distinguish both sources of error. In other words, the state-based model is more informative. This can also be seen in the relative width of the two credible intervals for the two different approaches (state-based had a tighter interval). 
<br />
<br />
However, it is important to remember that both these models are based on an oversimplified process model. To make these projections more realistic there are a few methods we could investigate. One example to increase precision would be to add other methods of influence to the process model. For example, expanding the model to include the effects of water temperature and predation within the spawner-recruit relationship. This would increase the complexity of the model but would likely lead to decreased process error.
<br />

## Discussion

<br />
Throughout this paper we generated a simple set of spawner-recruit data and fit a Beverton-Holt curve using both a state-based approach and a direct model. Then, we varied the process variability in the generated data to see the differences with each approach.
<br />
<br />
Ultimately, what we saw was that a lack of incorporation of both process and observation error within a model can lead to increased variability in parameter estimation. Furthermore, even with an abundance of data, models which do not take into account both sources of error can be highly uncertain and be extremely sensitive to model assumptions [@clark2004population]. The biggest issue of keeping models simple is being incapable of capturing larger complexities. While a simple model may be a great fit for the collected data, often, as seen by the uncertainty, will not do a good job of modeling the true relationship. When we raised process variability, we saw exactly this. While both models had increased variability in the parameter posteriors, the state-based approach was able to capture the process variability. In other words, the ability to capture these different forms of error is what makes the state-based approach superior to the simple model.
<br />
  Using this framework as a basis, we can now expand our discussion. The following are a few next steps for further investigation:
<br />
1. Modelling populations with age structures.
<br />
2. Model miss-specification. This includes using the incorrect spawner-recruit model and modeling multiple sub populations as one.
<br />
3. Exploring the benefits of adding and collecting smolt data. This would remove the variability of ocean survival which would in turn reduce process error.
<br />
4. Investigate how getting measurements and estimates of observation error impacts the model.
<br />
5. Fitting the model to less informative data. This would entail altering harvest rate and pHos so that we don't have as large of a spread of data values.
<br />
6. Reduce the number of years of data provided. This would be similar to less informative data.
<br />
7. Simulate multiple N year data sets and fit each to determine average performance.
<br />

## References

